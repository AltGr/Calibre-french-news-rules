# -*- mode:python -*-

__license__   = 'GPL v3'
__copyright__ = '2009, Mathieu Godlewski <mathieu at godlewski.fr>; 2010-2015, Louis Gesbert <meta at antislash dot info>'
'''
Mediapart
'''

__author__ = '2009, Mathieu Godlewski <mathieu at godlewski.fr>; 2010-2015, Louis Gesbert <meta at antislash dot info>'

import re
from calibre.ebooks.BeautifulSoup import BeautifulSoup, Tag
from calibre.web.feeds.news import BasicNewsRecipe
from calibre.web.feeds import feeds_from_index
from datetime import datetime,date,timedelta

class Mediapart(BasicNewsRecipe):
    title = 'Mediapart'
    __author__ = 'Mathieu Godlewski, Louis Gesbert'
    description = 'Global news in french from news site Mediapart'
    publication_type = 'newspaper'
    language = 'fr'
    needs_subscription = True

    # date in the feed is unreliable, we filter afterwards
    oldest_article = 100.0

    use_embedded_content = False
    no_stylesheets = True

    cover_url = 'https://static.mediapart.fr/files/M%20Philips/logo-mediapart.png'

# --

    custom_oldest_article = 2.0
    oldest_article_date = date.today() - timedelta(days=custom_oldest_article)

    feeds =  [
        ('La Une', 'https://www.mediapart.fr/articles/feed'),
    ]

    def parse_feeds(self):
        feeds = super(Mediapart, self).parse_feeds()
        feeds += feeds_from_index(self.my_parse_index(feeds))
        return feeds

    def my_parse_index(self, la_une):
        articles = []

        fils = {
            "Brève": [],
            "Lien": [],
            "Confidentiel": [],
            "Live": [],
            "Document": [],
            }

        def read_fil_index(url):
            soup = self.index_to_soup(url)

            for article in soup.findAll('li', {'data-type': 'article'}):
                try:
                    title = article.find('h3', 'title', recursive=False)
                    if title == None or title['class'] == 'title-specific': continue

                    article_type = article.find('a', {'href': re.compile(r'.*\/type-darticles\/.*')}).renderContents()
                    if article_type not in fils: continue

                    for s in title('span'): s.replaceWith(s.renderContents() + "\n")
                    url = title.find('a', href=True)['href']

                    article_date = self.parse_french_date(article.find('div', 'author').renderContents())

                    if article_date < self.oldest_article_date: continue

                    authors = article.findAll('a',{'class':re.compile(r'\bjournalist\b')})
                    authors = [ self.tag_to_string(a) for a in authors ]

                    description = article.find('div', {'class': lambda c: c != 'taxonomy-teaser'}, recursive=False).findAll('p')

                    summary = {
                        'title': self.tag_to_string(title).strip(),
                        'author': ', '.join(authors),
                        'url': url,
                        'date': u'' + article_date.strftime("%A %d %b %Y"),
                        'description': '\n'.join([self.tag_to_string(d) for d in description]),
                        }
                    fils[article_type].append(summary)
                except: pass

        pages = 3
        for page in xrange(0, pages-1):
            read_fil_index('https://www.mediapart.fr/journal/fil-dactualites?page=%d' % page)

        for (name,contents) in fils.iteritems():
            if contents: articles += [(name+'s', contents)]

        return articles

# -- print-version

    conversion_options = { 'smarten_punctuation' : True }

    remove_tags = [ dict(name='div', attrs={'class':'print-source_url'}) ]

    # non-locale specific date parse (strptime("%d %b %Y",s) would work with french locale)
    def parse_french_date(self, date_str):
        date_arr = date_str.lower().split()
        months = ['janvier', 'février', 'mars', 'avril', 'mai', 'juin',
                  'juillet', 'août', 'septembre', 'octobre', 'novembre',
                  'décembre']
        def get_month(s):
            s = s.partition(".")[0]
            for m in range(0,12):
                if months[m].startswith(s):
                    return m + 1
        return date(day=int(date_arr[0]),
                    year=int(date_arr[2]),
                    month=get_month(date_arr[1]))

    def print_version(self, url):
        raw = self.browser.open(url).read()
        soup = BeautifulSoup(raw.decode('utf8', 'replace'))

        # Filter old articles
        article_date = self.parse_french_date(soup.find('div', 'author').renderContents())

        if article_date < self.oldest_article_date:
           return None

        tools = soup.find('div', {'id':'menuOutilsTopEl'})
        link = tools.find('a', {'href': re.compile(r'\/print\/.*')})
        if link is None:
            print 'Error: print link not found'
            return None
        return 'https://www.mediapart.fr' + link['href']


# -- Handle login

    def get_browser(self):
        br = BasicNewsRecipe.get_browser(self)
        br.set_handle_gzip(True)
        if self.username is not None and self.password is not None:
            br.open('https://www.mediapart.fr/login')
            br.select_form(nr=1)
            br['name'] = self.username
            br['password'] = self.password
            br.submit()
        return br

    # This is a workaround articles with scribd content that include
    # <body></body> tags _within_ the body
    preprocess_regexps = [
        (re.compile(r'(<body.*?>)(.*)</body>', re.IGNORECASE|re.DOTALL),
         lambda match:
             match.group(1)
             + re.sub(re.compile(r'</?body>', re.IGNORECASE|re.DOTALL),'',
                      match.group(2))
             + '</body>')
    ]

    def preprocess_html(self, soup):
        mediapart_line = soup.body.find(text=re.compile(r"- Mediapart.fr"))
        mediapart_line and mediapart_line.extract()
        content = soup.body.find('div', {'class': None}, recursive=False)
        lire_aussi = content.find('p', text=re.compile(r"\[\[lire_aussi\]\]"))
        lire_aussi and lire_aussi.extract()
        abstract = content.p
        qu = Tag(soup,'blockquote')
        content.insert(0,qu)
        qu.insert(0,abstract)
        return soup
